{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c92fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import tiktoken\n",
    "import torch\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from classes import GPTDatasetV1\n",
    "import fitz  # PyMuPDF\n",
    "# Load PDF\n",
    "doc = fitz.open(r\"C:\\Users\\ASISH\\Downloads\\LLM\\wharton_verdict.pdf\")\n",
    "\n",
    "# Extract all text\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text += page.get_text()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56812f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader for GPT\n",
    "def create_dataloader_v1(text, batch_size=4, max_length=200, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(text, tokenizer, max_length, stride)\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "X=create_dataloader_v1(text=text, batch_size=4, max_length=200, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52755171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "def embedding(dataloader):\n",
    "\n",
    "    # token embedding\n",
    "    data_iter = iter(dataloader)\n",
    "    input_batch, target_batch = next(data_iter)\n",
    "    # GPT-2 vocab size\n",
    "    vocab_size = 50257 \n",
    "    # typical embedding size \n",
    "    embedding_dim = 768  \n",
    "    torch.manual_seed(123)\n",
    "    token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "    token_embeddings = token_embedding_layer(input_batch)\n",
    "\n",
    "    #positional embedding\n",
    "    max_length = 200\n",
    "    context_length = max_length\n",
    "    pos_embedding_layer = torch.nn.Embedding(context_length,embedding_dim)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "    \n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "    return input_embeddings\n",
    "\n",
    "Y=embedding(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8480d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in = embedding(X).shape[2] #B 768\n",
    "d_out = 500 #C whatever i want based on model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a91533dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c23dafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casual attention \n",
    "\n",
    "def self_attention (input_embeddings):\n",
    "    seq_len = 20 \n",
    "    input_embeddings = torch.randn(seq_len, d_in)  # Shape: (seq_len, d_in)\n",
    "    batch = torch.stack((input_embeddings, input_embeddings), dim=0)\n",
    "\n",
    "    torch.manual_seed(123)\n",
    "    context_length = batch.shape[1]\n",
    "    ca = CausalAttention(d_in, d_out, context_length=context_length, dropout=0.1)\n",
    "    context_vecs = ca(batch)   \n",
    "    return context_vecs\n",
    "\n",
    "Z=self_attention(Y)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53055fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2321e+00,  2.7056e-01,  1.2016e+00,  ...,  1.1351e-01,\n",
       "           2.1959e-01, -1.7183e+00],\n",
       "         [ 1.6240e+00,  3.8437e-02,  6.6129e-01,  ...,  1.5938e-01,\n",
       "          -3.0147e-02, -1.4106e+00],\n",
       "         [ 4.3470e-01, -5.4729e-02,  1.0739e-01,  ...,  3.5351e-02,\n",
       "          -1.9121e-01, -4.4711e-01],\n",
       "         ...,\n",
       "         [ 2.1222e-01,  6.1133e-02, -1.5091e-01,  ...,  1.4022e-01,\n",
       "          -1.1286e-01,  6.0857e-02],\n",
       "         [ 1.4445e-01,  5.6152e-02, -1.0781e-01,  ...,  1.6612e-01,\n",
       "          -1.6136e-01, -6.5314e-02],\n",
       "         [ 1.8809e-01,  1.3874e-01, -8.1277e-04,  ...,  3.4327e-01,\n",
       "          -2.2253e-02,  1.3157e-01]],\n",
       "\n",
       "        [[ 2.2321e+00,  2.7056e-01,  1.2016e+00,  ...,  1.1351e-01,\n",
       "           2.1959e-01, -1.7183e+00],\n",
       "         [ 1.6240e+00,  3.8437e-02,  6.6129e-01,  ...,  1.5938e-01,\n",
       "          -3.0147e-02, -1.4106e+00],\n",
       "         [ 1.0430e+00,  1.8998e-02,  4.3483e-01,  ...,  6.6284e-02,\n",
       "          -1.3137e-01, -9.1536e-01],\n",
       "         ...,\n",
       "         [ 1.3341e-01,  6.8854e-02, -1.8312e-01,  ...,  2.1652e-01,\n",
       "          -1.1585e-01,  1.3889e-01],\n",
       "         [ 1.6728e-01,  1.4692e-01, -9.8696e-02,  ...,  3.0205e-01,\n",
       "          -2.7361e-01,  3.5278e-02],\n",
       "         [ 1.7210e-01,  1.5232e-01, -9.1334e-02,  ...,  2.4553e-01,\n",
       "          -3.5776e-02,  6.2212e-02]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "afcdb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector db\n",
    "\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def vector_db(context_vecs):\n",
    "    # Convert torch.Tensor or list of vectors to numpy float32 array\n",
    "    if isinstance(context_vecs, torch.Tensor):\n",
    "        context_vecs = context_vecs.detach().cpu().numpy()\n",
    "    elif isinstance(context_vecs, list):\n",
    "        context_vecs = np.array(context_vecs)\n",
    "    \n",
    "    context_vecs = context_vecs.astype(\"float32\")\n",
    "\n",
    "    # Validate shape\n",
    "    if context_vecs.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D array (n, d), but got shape {context_vecs.shape}\")\n",
    "\n",
    "    # Create dummy documents\n",
    "    documents = [Document(page_content=f\"doc_{i}\") for i in range(len(context_vecs))]\n",
    "\n",
    "    # Create FAISS index and add vectors\n",
    "    index = faiss.IndexFlatL2(context_vecs.shape[1])\n",
    "    index.add(context_vecs)\n",
    "\n",
    "    # Build docstore and mapping\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "\n",
    "    # Return FAISS vectorstore\n",
    "    vector_store = FAISS(index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b5bfdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2, 20, 500)\n"
     ]
    }
   ],
   "source": [
    "print(type(Z))\n",
    "print(np.array(Z).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcb82672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_db(context_vecs):\n",
    "    if isinstance(context_vecs, torch.Tensor):\n",
    "        context_vecs = context_vecs.detach().cpu().numpy().astype(\"float32\")\n",
    "    elif isinstance(context_vecs, list):\n",
    "        context_vecs = np.array(context_vecs).astype(\"float32\")\n",
    "\n",
    "    if context_vecs.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D array (n, d), but got shape {context_vecs.shape}\")\n",
    "\n",
    "    # Create dummy docs\n",
    "    from langchain.docstore.document import Document\n",
    "    documents = [Document(page_content=f\"doc_{i}\") for i in range(len(context_vecs))]\n",
    "\n",
    "    import faiss\n",
    "    index = faiss.IndexFlatL2(context_vecs.shape[1])\n",
    "    index.add(context_vecs)\n",
    "\n",
    "    from langchain.docstore import InMemoryDocstore\n",
    "    from langchain.vectorstores import FAISS\n",
    "\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "\n",
    "    return FAISS(index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b39f1a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array (n, d), but got shape (2, 20, 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mvector_db\u001b[39m\u001b[34m(context_vecs)\u001b[39m\n\u001b[32m      5\u001b[39m     context_vecs = np.array(context_vecs).astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context_vecs.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected 2D array (n, d), but got shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_vecs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Create dummy docs\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n",
      "\u001b[31mValueError\u001b[39m: Expected 2D array (n, d), but got shape (2, 20, 500)"
     ]
    }
   ],
   "source": [
    "vector_db(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5987d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(Z, torch.Tensor):\n",
    "    context_vecs = Z.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "254de51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vecs = Z.reshape(-1, Z.shape[-1])  # (2*20, 500) → (40, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96e5c07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 500)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51c6ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_db(context_vecs):\n",
    "    if isinstance(context_vecs, torch.Tensor):\n",
    "        context_vecs = context_vecs.detach().cpu().numpy()\n",
    "\n",
    "    context_vecs = context_vecs.reshape(-1, context_vecs.shape[-1])\n",
    "\n",
    "    documents = [Document(page_content=\"dummy\")] * len(context_vecs)  # Replace later\n",
    "    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})\n",
    "    index_to_docstore_id = {i: str(i) for i in range(len(documents))}\n",
    "\n",
    "    index = faiss.IndexFlatL2(context_vecs.shape[1])\n",
    "    index.add(context_vecs)\n",
    "\n",
    "    vector_store = FAISS(index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f86be0f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "vector_db() got an unexpected keyword argument 'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mvector_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_vecs\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: vector_db() got an unexpected keyword argument 'embedding'"
     ]
    }
   ],
   "source": [
    "vector_db(context_vecs,embedding=embedding)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
